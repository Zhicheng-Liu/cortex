PROPOSAL FOR MARZAM

- What is it?
Marzam is a (new) memory efficient implementation of de Bruijn Graphs opening up possibilities for comparing multiple genomes, de novo assembling, resequencing and variation analysis. Marzam approach relies on an a data structure optimised to represent DNA sequences.

The following possbilities are particularly well targeted for a 1000genomes R01 application

1. The stated goal of the 1000 genomes project is to find common shared variation.

"The scientific goals of the 1000 Genomes Project are to produce a catalog of variants that are present at 1 percent or greater frequency in the human population across most of the genome, and down to 0.5 percent or lower within genes."

Currently noone is approaching this question directly in a way that can embrace both small and large variations. It is currently impossible to load even the 30X coverage of one human genome into velvet - I think Ewan estimated this would take over 1 Terabyte of RAM. Marzam can load 15x in just 140Gb of RAM, and by that point, RAM usage is flattening off. On a machine with 256 Gb of RAM, we should be able to load multiple deep genomes, and even the full data of the 1000 genomes project. Once this barrier is broken, we are then able todo the following:


1) Make direct calls on shared variation between individuals

We have a variety of potential algorithms here

--- Crude things we can do rapidly (i.e. not til January, given our current commitments), for demo purposes
..essentially - load people sequentially, and keep good contigs that are shared by >X people.

-- Exciting things we ought to be able to do:
Find contigs that are not in the reference but are in all the Yoruban trio, or all the Tuscans.
Find contigs that are in CEPH but not in Yoruba.
Find contigs that are in 25,50,75% of the 1000genomes individuals.

--- More sophisticated approaches

- We can make better use of the reference genome, by applying an energy minimisation approach. this has the benefit of being farm-able.
   Look at each chromosome separately. Break each reference chromosome into chunks, where the start and finish of each chunk has good coverage with reads
  with depth=expected depth. Then for each of these chunks, use a different farm node. For each one, the rule of the game is this. We don't want to be mucking
  around with contigs. We have a prior - the reference, and on the basis of our data (the reads), we deform the reference (ie this bit of our path through the graph)
  if by doing so we end up with better coverage, or better paired-end properties, than before. Concretely, we define an energy function that penalises you for having low coverage
  and for having reads that are the wrong distance from their mates. By changing the paramterisation, you should be able to do anything between only allowing SNPs (v strong penalisation
  for differing from reference), to allowing full scale SV's.




-- Infrastructure

-One of the lessons learned from trying to do targeted velvet work for finding SV's, was that we need to recalibrate our coverage to allow for GC bias - the Durbin, Hurles and Eichler
groups all agree this is crucial. So we need to be able to analyse data per library, and then adjust our coverages to calibrate for the fact that we undercall high-GC areas.

- I think you really want to load an individual, clean that person up, then merge them in.


-- problems we need to overcome

- the energy approach will need to handle places where chromosomes overlap.
- I would  like to find a better way of handling/encoding paired mates.

- Example of how energy alogirhtm might work:
		- Look at one chunk of chromosome. Move along it, and see if you find an exit supported by reads. If yes, follow it. If it returns to reference(*) in X steps, then calculate cost benefit.
			(*) - decide in advance f we need to return between our start/end poits, or same chromosome, or anywhere
